{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Machine Translation\n",
        "---\n",
        "make application to translate between English and Frensh \n"
      ],
      "metadata": {
        "id": "DDprj3dZUROR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps :-\n",
        "<ol>\n",
        "  <li>Import Dependanices</li>\n",
        "  <li>import dataset for training</li>\n",
        "  <li>cleaning data</li>\n",
        "  <li>tokenization and build vocabalury</li>\n",
        "  <li>Pad Sequence and Vectorization</li>\n",
        "  <li>Design LSTM NN (Encoder & Decoder)</li>\n",
        "  <li>train model with (50,100,200) epochs</li>\n",
        "  <li>build reference and Predictions</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "AbjL5aeHUi2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Dependanices"
      ],
      "metadata": {
        "id": "GOZdWqiHVimc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufHXf0z1Sbvt"
      },
      "outputs": [],
      "source": [
        "# basic libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# cleaning data\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "# save vocabulary in files\n",
        "import pickle\n",
        "\n",
        "# tokenization\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Model\n",
        "from tensorflow.keras.layers import LSTM,Embedding,Input,Dense,SpatialDropout1D,Activation\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "\n",
        "# training model dependanices\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import dataset for training"
      ],
      "metadata": {
        "id": "4ZOy5ptVYIhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/eng_-french.csv\")\n",
        "df.columns=[\"english\",\"frensh\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tD1TqeriUQCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "jgCj-X6-YWQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "take first 20000 rows from data to save memory in our future we can depand on batches and make all data"
      ],
      "metadata": {
        "id": "J7bWm90KYvXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=df[:]\n",
        "data.info()"
      ],
      "metadata": {
        "id": "lZ9G3HpfYbg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cleaning data"
      ],
      "metadata": {
        "id": "zsXos_d9ZKX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean english column\n",
        "def clean_english(text):\n",
        "  text=text.lower() # lower case\n",
        "\n",
        "  # remove any characters not a-z and ?!,'\n",
        "  text=re.sub(u\"[^a-z!?',]\",\" \",text)\n",
        "\n",
        "  # word tokenization\n",
        "  text=nltk.word_tokenize(text)\n",
        "\n",
        "  # join text\n",
        "  text=\" \".join([i.strip() for i in text])\n",
        "\n",
        "  return text\n",
        "clean_english(data.iloc[0,0])"
      ],
      "metadata": {
        "id": "0FvZAMWIZJNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[1,0],clean_english(data.iloc[1,0])"
      ],
      "metadata": {
        "id": "EwF-NcsuZJZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean frensh language\n",
        "def clean_frensh(text):\n",
        "  text=text.lower() # lower case\n",
        "\n",
        "  # remove any characters not a-z and ?!,'\n",
        "  # characters a-z and (éâàçêêëôîû) chars of frensh lang which contain accent\n",
        "  text=re.sub(u\"[^a-zéâàçêêëôîû!?',]\",\" \",text)\n",
        "\n",
        "  return text\n",
        "clean_frensh(data.iloc[0,1])"
      ],
      "metadata": {
        "id": "Ao58qisGbgDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[4,1],clean_frensh(data.iloc[4,1])"
      ],
      "metadata": {
        "id": "3T5h0kKob5uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[6,1],clean_frensh(data.iloc[6,1])"
      ],
      "metadata": {
        "id": "GbCd6CnLeNuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i show this two functions are ready to apply in dataframe\n",
        "data[\"english\"]=data[\"english\"].apply(lambda txt:clean_english(txt))\n",
        "data[\"frensh\"]=data[\"frensh\"].apply(lambda txt:clean_frensh(txt))"
      ],
      "metadata": {
        "id": "bzzD5VqggW2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add <start> <end> token to decoder sentence (Frensh)\n",
        "data[\"frensh\"]=data[\"frensh\"].apply(lambda txt:f\"<start> {txt} <end>\")"
      ],
      "metadata": {
        "id": "bDmKaA7a_L4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(10)"
      ],
      "metadata": {
        "id": "93tMogRbhCFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenization and build vocabalury"
      ],
      "metadata": {
        "id": "D-RUSc8kia0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# english tokenizer\n",
        "english_tokenize=Tokenizer(filters='#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
        "english_tokenize.fit_on_texts(data[\"english\"])"
      ],
      "metadata": {
        "id": "TDysGVCIidbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_encoder_tokens=len(english_tokenize.word_index)\n",
        "num_encoder_tokens"
      ],
      "metadata": {
        "id": "dtlhIcSkFb-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=english_tokenize.texts_to_sequences(data[\"english\"])\n",
        "encoder[:5]"
      ],
      "metadata": {
        "id": "TvBpI1ntFxEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_encoder_sequence_len=np.max([len(enc) for enc in encoder])\n",
        "max_encoder_sequence_len"
      ],
      "metadata": {
        "id": "XFwKUVpCF8tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frensh tokenizer\n",
        "french_tokenize=Tokenizer(filters=\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n\")\n",
        "french_tokenize.fit_on_texts(data[\"frensh\"])"
      ],
      "metadata": {
        "id": "GwNpJBpPGQ6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_decoder_tokens=len(french_tokenize.word_index)\n",
        "num_decoder_tokens"
      ],
      "metadata": {
        "id": "EEZ7by3DGq31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=french_tokenize.texts_to_sequences(data[\"frensh\"])\n",
        "decoder[:5]"
      ],
      "metadata": {
        "id": "eT52cpg4G6K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_decoder_sequence_len=np.max([len(dec) for dec in decoder])\n",
        "max_decoder_sequence_len"
      ],
      "metadata": {
        "id": "1VBW-lbaHDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_txt_decoder={k:i for i,k in french_tokenize.word_index.items()}\n",
        "idx_2_txt_decoder[1]"
      ],
      "metadata": {
        "id": "7ADjOexqHDhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_txt_encoder={k:i for i,k in english_tokenize.word_index.items()}\n",
        "idx_2_txt_encoder[2]"
      ],
      "metadata": {
        "id": "pZGfFzT7HDiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_txt_decoder[0]=\"<pad>\"\n",
        "idx_2_txt_encoder[0]=\"<pad>\""
      ],
      "metadata": {
        "id": "a7eUvxljiR2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save idx_2_txt_encoder and idx_2_txt_decoder , english_tokenize.word_index , frensh_tokenize.word_index\n",
        "pickle.dump(english_tokenize.word_index,open(\"/content/drive/MyDrive/word_2_idx_input.txt\",\"wb\"))\n",
        "pickle.dump(french_tokenize.word_index,open(\"/content/drive/MyDrive/word_2_idx_target.txt\",\"wb\"))\n",
        "pickle.dump(idx_2_txt_encoder,open(\"/content/drive/MyDrive/idx_2_word_input.txt\",\"wb\"))\n",
        "pickle.dump(idx_2_txt_decoder,open(\"/content/drive/MyDrive/idx_2_word_target.txt\",\"wb\"))"
      ],
      "metadata": {
        "id": "WyL1UIawHDjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pad sequences "
      ],
      "metadata": {
        "id": "IEoT65sWQLh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_seq=pad_sequences(encoder,maxlen=max_encoder_sequence_len,padding=\"post\")\n",
        "encoder_seq.shape"
      ],
      "metadata": {
        "id": "YPu3Ti8wHDkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inp=pad_sequences([arr[:-1] for arr in decoder],maxlen=max_decoder_sequence_len,padding=\"post\")\n",
        "decoder_inp.shape"
      ],
      "metadata": {
        "id": "BbPYivwnHDmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output=pad_sequences([arr[1:] for arr in decoder],maxlen=max_decoder_sequence_len,padding=\"post\")\n",
        "decoder_output.shape"
      ],
      "metadata": {
        "id": "iXMjN0_3HDoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([idx_2_txt_decoder[i] for i in decoder_output[0]])"
      ],
      "metadata": {
        "id": "wUkg8n5CHDpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([idx_2_txt_encoder[i] for i in encoder_seq[0]])"
      ],
      "metadata": {
        "id": "BxksvuVOisoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_output_categorical=to_categorical(decoder_output,num_classes=num_decoder_tokens+1)"
      ],
      "metadata": {
        "id": "PGJXIfW0HDqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_output_categorical.shape"
      ],
      "metadata": {
        "id": "tbRRE1RIHDrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design LSTM NN (Encoder & Decoder)"
      ],
      "metadata": {
        "id": "yQjGCEehkg4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "encoder_input=Input(shape=(None,),name=\"encoder_input_layer\")\n",
        "encoder_embedding=Embedding(num_encoder_tokens,300,input_length=max_encoder_sequence_len,name=\"encoder_embedding_layer\")(encoder_input)\n",
        "encoder_lstm=LSTM(256,activation=\"tanh\",return_sequences=True,return_state=True,name=\"encoder_lstm_1_layer\")(encoder_embedding)\n",
        "encoder_lstm2=LSTM(256,activation=\"tanh\",return_state=True,name=\"encoder_lstm_2_layer\")(encoder_lstm)\n",
        "_,state_h,state_c=encoder_lstm2\n",
        "encoder_states=[state_h,state_c]"
      ],
      "metadata": {
        "id": "XifK1UI-kEN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder model\n",
        "decoder_input=Input(shape=(None,),name=\"decoder_input_layer\")\n",
        "decoder_embedding=Embedding(num_decoder_tokens,300,input_length=max_decoder_sequence_len,name=\"decoder_embedding_layer\")(decoder_input)\n",
        "decoder_lstm=LSTM(256,activation=\"tanh\",return_state=True,return_sequences=True,name=\"decoder_lstm_layer\")\n",
        "decoder_outputs,_,_=decoder_lstm(decoder_embedding,initial_state=encoder_states)\n",
        "decoder_dense=Dense(num_decoder_tokens+1,activation=\"softmax\",name=\"deocer_final_layer\")\n",
        "outputs=decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "JUd663ZKoTeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Model([encoder_input,decoder_input],outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "np-eNY2Epbpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model with (50,100,200) epochs"
      ],
      "metadata": {
        "id": "QFLqImwSqhJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_seq.shape,decoder_inp.shape,decoder_output.shape"
      ],
      "metadata": {
        "id": "OliyQjINspM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "history=model.fit(\n",
        "    [encoder_seq,decoder_inp],\n",
        "    decoder_output,\n",
        "    epochs=25,\n",
        "    batch_size=256,\n",
        "    # callbacks=[callback]\n",
        ")"
      ],
      "metadata": {
        "id": "XptJGmvxqpK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Model_english_to_frensh.h5\")"
      ],
      "metadata": {
        "id": "Wuj585aJjVRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build reference and Predictions"
      ],
      "metadata": {
        "id": "pIqJMIcpVj27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_references():\n",
        "  encoder_reference_model=Model(encoder_input,encoder_states)\n",
        "\n",
        "  decoder_state_h=Input(shape=(256,))\n",
        "  decoder_state_c=Input(shape=(256,))\n",
        "  decoder_input_states=[decoder_state_h,decoder_state_c]\n",
        "\n",
        "  decoder_outputs,state_h,state_c=decoder_lstm(decoder_embedding,initial_state=decoder_input_states)\n",
        "\n",
        "  decoder_state=[state_h,state_c]\n",
        "  decoder_outputs=decoder_dense(decoder_outputs)\n",
        "  decoder_reference_model=Model([decoder_input]+decoder_input_states,[decoder_outputs]+decoder_state)\n",
        "\n",
        "  return encoder_reference_model,decoder_reference_model"
      ],
      "metadata": {
        "id": "qeqVbRonjmyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_text(text):\n",
        "  text=clean_english(text)\n",
        "\n",
        "  res=[english_tokenize.word_index[i] for i in text.split(\" \")]\n",
        "  pad=pad_sequences([res],maxlen=max_encoder_sequence_len,padding=\"post\")\n",
        "  return pad\n",
        "prepare_text(\"How are you\")"
      ],
      "metadata": {
        "id": "ZHH9thA2Yw6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model,dec_model=make_references()\n",
        "\n",
        "states_value=enc_model(prepare_text(input(\"Enter text :- \")))\n",
        "\n",
        "empty_target_seq=np.zeros((1,1))\n",
        "empty_target_seq[0,0]=french_tokenize.word_index[\"start\"]\n",
        "\n",
        "stop_condition=False\n",
        "decoded_translaition=\"\"\n",
        "\n",
        "while not stop_condition:\n",
        "  dec_output,h,c=dec_model.predict([empty_target_seq]+states_value)\n",
        "  sampled_word_index=np.argmax(dec_output[0,-1,:])\n",
        "  sampled_word=None\n",
        "\n",
        "  for word,index in french_tokenize.word_index.items():\n",
        "    if sampled_word_index == index:\n",
        "      decoded_translaition+=' {}'.format(word)\n",
        "      sampled_word=word\n",
        "\n",
        "    if sampled_word == \"end\" or len(decoded_translaition.split(\" \")) >= max_decoder_sequence_len:\n",
        "      stop_condition=True\n",
        "\n",
        "  empty_target_seq=np.zeros((1,1))\n",
        "  empty_target_seq[0,0]=sampled_word_index\n",
        "  states_value=[h,c]\n",
        "print(decoded_translaition)"
      ],
      "metadata": {
        "id": "zhtcDle9aKO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}